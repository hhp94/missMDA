---
title: "modding"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{modding}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r}
get_imputed <- function(d) {
  purrr::map2_dbl(r_indx$n, r_indx$p, \(x, y) {
    if (d[x, y] < 0) {
      return(0)
    }
    if (d[x, y] > 1) {
      return(1)
    }
    return(d[x, y])
  })
}
```

# mod imputePCA
```{r setup}
library(profvis)
library(tictoc)
load_all()

set.seed(1234)

r_indx$mod_fit <- NULL
beta_miss |> dim()

tic()
mod_fit <- beta_miss |> methyl_imputePCA(ncp = 2)
toc()

r_indx$mod_fit <- get_imputed(mod_fit$completeObs)
r_indx

all(dplyr::near(r_indx$mod_fit, r_indx$after_ipca_ncp2))
```

```{r setup}
p <- profvis(methyl_imputePCA(beta_miss))
p1 <- profvis(imputePCA(beta_miss))

p
p1
```

## SVD
```{r}
library(Matrix)
library(rsvd)

Xhat <- readRDS("data-raw/Xhat.rds")
dim(Xhat)

n <- 49
microbenchmark::microbenchmark(
  # irlba::irlba(Xhat, nv = n),
  # FactoMineR::svd.triplet(Xhat, ncp = n),
  corpcor::fast.svd(Xhat),
  rsvd::rsvd(Xhat, n),
  times = 10
)

ncp_est <- estim_ncpPCA(beta_miss, ncp.max = 20)
```

* `corpcor::fast.svd(Xhat)` Seems to be offer the best trade off if ncp can range 
from 2 to whatever. 
* `irlba::irlba` iterative is actually the fastest if d is small enough. This 
is usually the case for n < 10

```{r}
Xhat <- readRDS("data-raw/Xhat.rds")
n <- 3
svdeez <- list()

svd(Xhat, n, n)
irlba_wrapper(Xhat, n, n)

svdeez$svd <- svd(Xhat, nu = n, nv = n)
svdeez$svd |> lapply(dim)
round(svdeez$svd$d, 4)

svdeez$default <- FactoMineR::svd.triplet(Xhat, ncp = n)
svdeez$default |> lapply(dim)

svdeez$irlba <- irlba::irlba(Xhat, nv = n)
svdeez$irlba |> lapply(dim)
svdeez$irlba$d

svdeez$corpo <- corpcor_wrapper(Xhat, n)
svdeez$corpo |> lapply(dim)

svdeez$irlba$u
waldo::compare(
  svdeez$irlba$u,
  svdeez$corpo$U
)
```

* Seed should be set outside of function
* We don't need the row weights so we can just use colMeans/colSDs for speed up
* If we always use scaling, then we can replace a lot of calls and checks
* We don't need quali sup because all the technical errors are already observed
in the cpg matrix

Change
* `estim_ncpPCA` doesn't use `ind.sub` and `quali.sub` and `quanti.sup` anyway, 
so we remove them.    
* Looks like seed is `NULL` as default. So we can just remove it

```{r}
sum(1:10)
res <- sum(V * poids, na.rm = TRUE) / sum(poids[!is.na(V)])
```

Old name space
```{r}
# export(estim_ncpFAMD, estim_ncpPCA, estim_ncpMCA, estim_ncpMultilevel, imputeCA, imputeFAMD, imputeMCA, imputeMFA, imputeMultilevel, imputePCA, MIMCA, MIFAMD, MIPCA, Overimpute, prelim, plot.MIPCA, plot.MIMCA)
# import(doParallel)
# import(foreach)
# import(parallel)
# importFrom(FactoMineR, PCA, MCA, svd.triplet, coord.ellipse, reconst, tab.disjonctif.prop, plot.PCA)
# importFrom(ggplot2, aes, geom_point, guides, guide_legend)
# importFrom(graphics, plot, points, abline, segments, legend, par, mtext)
# importFrom(grDevices, dev.new, heat.colors)
# importFrom(mice, as.mids)
# importFrom(mvtnorm, rmvnorm)
# importFrom(stats, var, sd, rnorm, quantile, aggregate)
# importFrom(utils, txtProgressBar, setTxtProgressBar)
#
# S3method(plot, MIPCA)
# S3method(plot, MIMCA)
```

# mod estim_ncpPCA
```{r}
library(tictoc)
load_all()

set.seed(1234)

beta_miss |> dim()

tic()
estim <- estim_ncpPCA(beta_miss, ncp.min = 10, ncp.max = 10, method.cv = "Kfold")
toc()

estim

tic()
estim2 <- methyl_estim_ncpPCA(beta_miss, ncp.min = 2, ncp.max = 15, maxiter = 2000)
toc()

waldo::compare(estim, estim2, tolerance = .Machine$double.eps^0.5)

# R> data("tiger", package = "rsvd")
data("tiger", package = "rsvd")
image(tiger, col = gray(0:255 / 255))
tiger |> dim()
```

```{r}
load_all()
library(furrr)
library(profvis)
plan(multisession, workers = 6)

set.seed(12345)
test <- kfold_estim_ncpPCA(beta_miss, 1, furrr_options(seed = TRUE), nbsim = 200)
test$job

prof <-
  profvis({
    kfold_estim_ncpPCA(beta_miss, 2, furrr_options(seed = TRUE), nbsim = 200)
  })

prof
plan(sequential)
```

# mod imputeMCA
Lots of questionnaires are very fat. We use the same trick of wrapping it in
fast svd to improve speed. We can sacrifice the supplementary 
individuals/variables for common use case of not having these variables

```{r}
data(vnf)
# There are rows with all missing for vnf
vnf |> head()
vnf[c(841,1073), ]
# vnf0 is for testing
vnf0 <- vnf[-c(841,1073), ]
vnf1 <- cbind(vnf, data.frame(kek = factor(NA)))
```

## moy.p
This is a scaled colSums by the number of non missing rows per column
- The number of non missing rows per column is fixed at the start so we only need to calculate it once instead of currently being calculated in every iteration.
- we can use colsums(x) to replace the nominator
- then colsums(!is.na(x)) to replace the denominator. This is calculated only once.

This is just sum over the columns by weight.

Results: We actually slow things down by using colSums instead of apply. This is
difficult to believe. But here we are

```{r}
moy.p <- function(V, poids) {
  res <- sum(V * poids, na.rm = TRUE) / sum(poids[!is.na(V)])
  res
}

row.weight <- rep(1 / nrow(vnf0), nrow(vnf0))
tabj.prop <- FactoMineR::tab.disjonctif.prop(vnf0, row.w = row.weight)
tabj.prop
moy.p(tabj.prop[, 4], row.weight)
```

## svd
`corpcor_wrap`/`bootSVD_wrap` is faster than svd for most cases since most of our data is tall or thin instead of square. 

We don't have to worry about signs because the author's svd.triplet handles this 

But we need to build unit tests for for both tall and wide cases

### test
```{r}
data(sim_tall)
data(sim_wide)

# test_case
for(i in 1:10) {
  t1 <- fit_compare_fns(sim_tall$results, ncp = i, svd_fns = "svd")
  print(sum(t1$test != t1$modded_test))
}

for(i in 1:10) {
  t1 <- fit_compare_fns(sim_tall$results, ncp = i, svd_fns = "bootSVD")
  print(sum(t1$test != t1$modded_test))
}

for(i in 1:10) {
  t1 <- fit_compare_fns(sim_wide$results, ncp = i, svd_fns = "bootSVD")
  print(sum(t1$test != t1$modded_test))
}
```

```{r}
library(microbenchmark)
microbenchmark::microbenchmark(
  imputeMCA(sim_wide$results, ncp = 3),
  modded_imputeMCA(sim_wide$results, ncp = 3, svd_fns = "corpcor"),
  modded_imputeMCA(sim_wide$results, ncp = 3, svd_fns = "bootSVD"),
  times = 10
)

microbenchmark::microbenchmark(
  modded_imputeMCA(sim_tall$results, ncp = 3, svd_fns = "svd"),
  modded_imputeMCA(sim_tall$results, ncp = 3, svd_fns = "corpcor"),
  modded_imputeMCA(sim_tall$results, ncp = 3, svd_fns = "bootSVD"),
  times = 30
)
```

# mod estim_ncpMCA
replace `imputeMCA` with `modded_imputeMCA` and also add the parallel over ncp
```{r}
library(progressr)
handlers(global = TRUE)
```

```{r}
prodna1(vnf0, 0.05)
debug(generate_k_fold)
generate_k_fold(vnf0, 0.99, max_iterations = 1)
undebug(generate_k_fold)
```

```{r}
debug(estim_ncpMCA_kfold)
estim_ncpMCA_kfold(
  vnf0,
  ncp.min = 2,
  ncp.max = 2,
  nbsim = 25,
  pNA = 0.05,
  threshold = 1e-4,
  verbose = TRUE
)
undebug(estim_ncpMCA_kfold)
```

## benchmark
### single core
About 1.239 times faster from the rewrites.
```{r}
estim_args <- list(
  don = vnf0,
  ncp.min = 2,
  ncp.max = 4,
  nbsim = 10,
  pNA = 0.01,
  threshold = 1e-4,
  verbose = TRUE
)

microbenchmark::microbenchmark(
  do.call("estim_ncpMCA", estim_args),
  do.call("modded_estim_ncpMCA", estim_args),
  times = 3
)
```

### multicore
Get up to X folds faster with parallel
```{r}
library(future.apply)
estim_args2 <- estim_args
estim_args2$nbsim <- 50
plan(multisession, workers = 3)
microbenchmark::microbenchmark(
  do.call("estim_ncpMCA", estim_args2),
  do.call("modded_estim_ncpMCA", estim_args2),
  times = 1
)
plan(sequential)
```


### debug
```{r}
# dummy code
debug(modded_imputeMCA)
modded_imputeMCA(vnf0)
undebug(modded_imputeMCA)
```
